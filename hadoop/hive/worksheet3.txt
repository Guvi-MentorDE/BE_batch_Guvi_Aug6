
bucketing:
------------
set hive.cli.print.header=true;

create table if not exists emp_dept_location 
(deptno int,
deptname string,
empname string,
sal int,
location string) 
row format delimited fields terminated by',' lines terminated by'\n'
stored as textfile;

load data local inpath '/home/Raj/data/hive_data/bucket/dept_loc.txt' into table emp_dept_location;


create table if not exists part_dept_location 
(deptno int,
empname string,
sal int,
location string) 
partitioned by (deptname string)            
clustered by(location) into 4 buckets        
row format delimited fields terminated by',' lines terminated by'\n'
stored as textfile;


dept = hr 
    - bucket 1 
    - bucket 2 
    - 3
    - 4


dept = fin 
    - bucket 1 
    - bucket 2 
    - 3
    - 4

set hive.exec.dynamic.partition.mode=nonstrict;
--default = 100 dynamic partitions / per datanode

insert into table part_dept_location partition(deptname)
select deptno, empname, sal, location, deptname from emp_dept_location;

show partitions part_dept_location;


select location , count(*)
from part_dept_location
where deptname='HR'    
and location in('Chandigarh','Houston','Denver','Pune') 
group by location;



Table sampling:
-----------------

select deptno, empname,sal,location from part_dept_location tablesample(bucket 1 out of 2 on location);

select deptno, empname,sal,location from part_dept_location tablesample(20 rows);


Joins in hive:
---------------

drop table emp_tab;
create table if not exists emp_tab (col1 int,col2 string,col3 string,col4 int,col5 int,col6 int,col7 string) row format delimited fields terminated by',' lines terminated by'\n'stored as textfile;

drop table dept_tab;
create table if not exists dept_tab (col1 int,col2 string,col3 string,col4 string) row format delimited fields terminated by',' lines terminated by'\n'stored as textfile;

load data local inpath '/home/Raj/data/hive_data/Join/employee' into table emp_tab;

load data local inpath '/home/Raj/data/hive_data/Join/dept' into table dept_tab;



JOINS:
------

set hive.cli.print.header=true;

select distinct emp_tab.col1,emp_tab.col2,emp_tab.col3,dept_tab.col1,dept_tab.col2,dept_tab.col3 
from emp_tab 
join dept_tab 
on (emp_tab.col6 = dept_tab.col1);


select emp_tab.col1,emp_tab.col2,emp_tab.col3,dept_tab.col1,dept_tab.col2,dept_tab.col3 
from emp_tab 
left join dept_tab 
on (emp_tab.col6 = dept_tab.col1);


select emp_tab.col1,emp_tab.col2,emp_tab.col3,dept_tab.col1,dept_tab.col2,dept_tab.col3 
from emp_tab 
full outer join dept_tab 
on (emp_tab.col6 = dept_tab.col1);


Multi level partition:
========================
[note: mostly not recommended.]


drop table airport_data_multilevel;
create table airport_data_multilevel(
passenger_id string,
First_Name string,
Last_Name string,
Gender string,
Age string,
Nationality string, 
Airport_Name string,
Country_Name string,
Departure_Date string,
Arrival_Airport string,
Pilot_Name string,
Flight_Status string
)
partitioned by(Continents string, Airport_Country_Code string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/tmp';

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=500;

Insert into table airport_data_multilevel partition (Continents, Airport_Country_Code)
select passenger_id,First_Name, Last_Name, Gender, Age, Nationality, Airport_Name, Country_Name, Departure_Date, Arrival_Airport, Pilot_Name, Flight_Status,Continents, Airport_Country_Code
from airport_data_raw;

select distinct Continents , Airport_Country_Code from airport_data_raw ;

select distinct Continents from airport_data_multilevel limit 10;

how to kill a job [execute at your own discretion]:
----------------------------------------------------
yarn application -kill application_1696045670780_0002




Performance optimizations:
=============================


Map join:
----------

select /*+ MAPJOIN (dept_tab) */ emp_tab.col1,emp_tab.col2,emp_tab.col3,dept_tab.col1,dept_tab.col2,dept_tab.col3 from emp_tab join dept_tab on (emp_tab.col6 = dept_tab.col1);

SET hive.auto.convert.join=true;
select emp_tab.col1,emp_tab.col2,emp_tab.col3,dept_tab.col1,dept_tab.col2,dept_tab.col3 from emp_tab join dept_tab on (emp_tab.col6 = dept_tab.col1);





Bucket Map Join
---------------

drop table automobile_sales_raw;
create table automobile_sales_raw ( ORDERNUMBER int, QUANTITYORDERED int, PRICEEACH float, ORDERLINENUMBER int, SALES float, STATUS string, QTR_ID int, MONTH_ID int, YEAR_ID int, PRODUCTLINE string, MSRP int, PRODUCTCODE string, PHONE string, CITY string, STATE string, POSTALCODE string, COUNTRY string, TERRITORY string, CONTACTLASTNAME string, CONTACTFIRSTNAME string, DEALSIZE string ) 
row format delimited fields terminated by ',' 
;

load data local inpath '/home/Raj/data/hive_data/sales_ecom.txt' into table automobile_sales_raw;

drop table automobile_sales_product_info;
create table automobile_sales_product_info (PRODUCTCODE string , MSRP int) clustered by (PRODUCTCODE) sorted by (PRODUCTCODE) into 5 buckets
stored as ORC 
tblproperties("orc.compress"="SNAPPY");

insert into table automobile_sales_product_info
select distinct PRODUCTCODE, MSRP from automobile_sales_raw;

drop table automobile_orders_sales;
create table automobile_orders_sales ( ORDERNUMBER int, QUANTITYORDERED int, PRICEEACH float, ORDERLINENUMBER int, SALES float, STATUS string,  PRODUCTCODE string, MSRP int) clustered by (PRODUCTCODE) sorted by (PRODUCTCODE) into 5 buckets
stored as ORC 
tblproperties("orc.compress"="SNAPPY");

insert into table automobile_orders_sales 
select distinct ORDERNUMBER, QUANTITYORDERED, PRICEEACH, ORDERLINENUMBER, SALES, STATUS, PRODUCTCODE, MSRP 
from automobile_sales_raw;


select a.* , b.* from 
automobile_orders_sales a 
join automobile_sales_product_info b 
on a.PRODUCTCODE = b.PRODUCTCODE;




sort merge join:
-----------------

set hive.enforce.sortmergebucketmapjoin=false; 
set hive.auto.convert.sortmerge.join=true; 
set hive.optimize.bucketmapjoin = true; 
set hive.optimize.bucketmapjoin.sortedmerge = true;


select a.* , b.* from 
automobile_orders_sales a 
join automobile_sales_product_info b 
on a.PRODUCTCODE = b.PRODUCTCODE;



SET hive.auto.convert.join=true;
set hive.optimize.bucketmapjoin = true; 

MR 

TEZ

two tables with bucketing , partitioning,
use colum based storeage like ORC , 
snappy compression
join optimization : map (join/buckting join) / sort merge join 


loading with serde:
--------------------

drop table automobile_sales_raw_serde;
create table automobile_sales_raw_serde ( ORDERNUMBER int, QUANTITYORDERED int, PRICEEACH float, ORDERLINENUMBER int, SALES float, STATUS string, QTR_ID int, MONTH_ID int, YEAR_ID int, PRODUCTLINE string, MSRP int, PRODUCTCODE string, PHONE string, CITY string, STATE string, POSTALCODE string, COUNTRY string, TERRITORY string, CONTACTLASTNAME string, CONTACTFIRSTNAME string, DEALSIZE string ) 
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
stored as orc 
;

load data local inpath '/home/Raj/data/hive_data/sales_ecom.txt' into table automobile_sales_raw_serde;


complier 

save he program ->  byte code after complining [program_name.jar]
ser , deser


how to kill a job:
-------------------

yarn application -kill application_1696045670780_0001


set hive.cbo.enable=true;
set hive.compute.query.using.stats=true;
set hive.stats.fetch.column.stats=true;
set hive.stats.fetch.partition.stats=true;
