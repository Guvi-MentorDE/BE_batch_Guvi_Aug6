spark-submit --master yarn  <program_name>.py 
spark-submit --master yarn  /home/Raj/bin/stream_cust_info.py

spark-submit \
 --master yarn \
 --deploy-mode client \
 --executor-memory 4G \
 --driver-memory 2G \
 --conf spark.app.name=cust_analytics \
 --conf spark.executor.cores=2 \
 --conf spark.executor.instances=5 \
 --conf spark.default.parallelism=20 \
 --conf spark.driver.maxResultSize=1G \
 --conf spark.network.timeout=800 \
/home/Raj/bin/cust_analytics.py


spark-submit \
 --master yarn \
 --deploy-mode client \
 --executor-memory 4G \
 --driver-memory 2G \
 --conf spark.app.name=cust_analytics \
 --conf spark.executor.cores=2 \
 --conf spark.executor.instances=5 \
 --conf spark.default.parallelism=20 \
 --conf spark.driver.maxResultSize=1G \
 --conf spark.network.timeout=800 \
/home/Raj/bin/authors_json.py


spark-submit \
 --master yarn \
 --deploy-mode cluster \
 --executor-memory 4G \
 --driver-memory 2G \
 --conf spark.app.name=cust_analytics \
 --conf spark.executor.cores=2 \
 --conf spark.executor.instances=5 \
 --conf spark.default.parallelism=20 \
 --conf spark.driver.maxResultSize=1G \
 --conf spark.network.timeout=800 \
/home/Raj/bin/authors_json.py


spark-submit \
 --master yarn \
 --deploy-mode client \
 --executor-memory 5G \
 --driver-memory 10G \
 --conf spark.app.name=authors_json_dynamic \
 --conf spark.executor.cores=2 \
 --conf spark.executor.instances=5 \
 --conf spark.default.parallelism=20 \
 --conf spark.driver.maxResultSize=1G \
 --conf spark.network.timeout=800 \
 --conf spark.sql.shuffle.partitions=100 \
 --conf spark.dynamicAllocation.enabled="true" \
 --conf spark.dynamicAllocation.initialExecutors=1 \
 --conf spark.dynamicAllocation.minExecutors=1 \
 --conf spark.dynamicAllocation.maxExecutors=8 \
/home/Raj/bin/log_analytics.py